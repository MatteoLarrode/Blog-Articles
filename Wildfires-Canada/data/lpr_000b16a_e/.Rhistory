vote16_20 <- vote_80_20 %>%
select(state, stateid, pctdem2016, pctdem2020)
vote_80_20_new <- vote_80_20 %>%
filter(state != "District of Columbia") %>%
pivot_longer(cols = starts_with("pctdem"),
names_to = "Year",
values_to = "vote_pct")%>%
select(state, stateid, Year, vote_pct)%>%
mutate(Year = str_extract(Year, "\\d{4}")) %>%
print(n = 15)
View(vote_80_20_new)
setwd("~/Desktop/Data Science/Machine_Learning")
knitr::opts_chunk$set(echo = TRUE)
#Prep Work & Packages
load("data/tweets.Rda")
setwd("~/Desktop/Data Science/Machine_Learning/Text_Data_Analysis")
knitr::opts_chunk$set(echo = TRUE)
#Prep Work & Packages
load("data/tweets.Rda")
load("data/tweets.Rda")
#Prep Work & Packages
load("data/tweets.Rda")
knitr::opts_chunk$set(echo = TRUE)
#Prep Work & Packages
load("data/tweets.Rda")
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(tidyverse)
library(glmnet)
library(ggplot2)
#Text Preparation ----------
tweetCorpus <- corpus(tweets$text, docvars = tweets)
#turn corpus into document-term matrix; make everything lower-case, remove numbers, punctuation and stopwords; TF-IDF weighting; remove airline names
dfm_tweets0 <- tweetCorpus %>%
tokens(remove_numbers=T,
remove_punct=T,
include_docvars=T,
remove_symbols = T) %>%
tokens_remove(stopwords("en")) %>%
dfm(tolower=T)%>%
dfm_tfidf()
dim(dfm_tweets0) #13,066 unique words
#eliminate rare words
dfm_tweets1 <- dfm_tweets0 %>% dfm_trim(min_docfreq = 5)
dim(dfm_tweets1)
#eliminate other words
dfm_tweets <- dfm_tweets1 %>% dfm_remove(c("@united", "@usairways", "@americanair", "@southwestair", "@jetblue", "@virginamerica", "flight", "amp", "aa", "get", "now", "united", "jetblue", "us"))
dim(dfm_tweets)
#Word Usage Analysis ----------
#overview
textstat_frequency(dfm_tweets, force=T)[1:15] #most used words
#turn corpus into document-term matrix; make everything lower-case, remove numbers, punctuation and stopwords; TF-IDF weighting; remove airline names
dfm_tweets0 <- tweetCorpus %>%
tokens(remove_numbers=T,
remove_punct=T,
include_docvars=T,
remove_symbols = T) %>%
tokens_remove(stopwords("en")) %>%
dfm(tolower=T)%>%
dfm_tfidf()
dim(dfm_tweets0) #13,066 unique words
print(paste0("Dimension of Initial Document-Term Matrix:", dim(dfm_tweets0)))
dim(dfm_tweets0)
dim(dfm_tweets0)[0]
dim(dfm_tweets0)[1]
dim(dfm_tweets0)[2]
#Text Preparation ----------
tweetCorpus <- corpus(tweets$text, docvars = tweets)
#turn corpus into document-term matrix; make everything lower-case, remove numbers, punctuation and stopwords; TF-IDF weighting; remove airline names
dfm_tweets0 <- tweetCorpus %>%
tokens(remove_numbers=T,
remove_punct=T,
include_docvars=T,
remove_symbols = T) %>%
tokens_remove(stopwords("en")) %>%
dfm(tolower=T)%>%
dfm_tfidf()
#eliminate rare words & other words
dfm_tweets <- dfm_tweets0 %>%
dfm_trim(min_docfreq = 5) %>%
dfm_remove(c("@united", "@usairways", "@americanair", "@southwestair", "@jetblue", "@virginamerica", "flight", "amp", "aa", "get", "now", "united", "jetblue", "us"))
print("Document-Term Matrix after pre-processing")
print(paste0("Number of tweets:", dim(dfm_tweets)[1])
print(paste0("Number of words:", dim(dfm_tweets)[2]))
#Text Preparation ----------
tweetCorpus <- corpus(tweets$text, docvars = tweets)
#turn corpus into document-term matrix; make everything lower-case, remove numbers, punctuation and stopwords; TF-IDF weighting; remove airline names
dfm_tweets0 <- tweetCorpus %>%
tokens(remove_numbers=T,
remove_punct=T,
include_docvars=T,
remove_symbols = T) %>%
tokens_remove(stopwords("en")) %>%
dfm(tolower=T)%>%
dfm_tfidf()
#eliminate rare words & other words
dfm_tweets <- dfm_tweets0 %>%
dfm_trim(min_docfreq = 5) %>%
dfm_remove(c("@united", "@usairways", "@americanair", "@southwestair", "@jetblue", "@virginamerica", "flight", "amp", "aa", "get", "now", "united", "jetblue", "us"))
print("Document-Term Matrix after pre-processing")
print(paste0("Number of tweets:", dim(dfm_tweets)[1]))
print(paste0("Number of words:", dim(dfm_tweets)[2])))
print(paste0("Number of words:", dim(dfm_tweets)[2]))
#Text Preparation ----------
tweetCorpus <- corpus(tweets$text, docvars = tweets)
#turn corpus into document-term matrix; make everything lower-case, remove numbers, punctuation and stopwords; TF-IDF weighting; remove airline names
dfm_tweets0 <- tweetCorpus %>%
tokens(remove_numbers=T,
remove_punct=T,
include_docvars=T,
remove_symbols = T) %>%
tokens_remove(stopwords("en")) %>%
dfm(tolower=T)%>%
dfm_tfidf()
#eliminate rare words & other words
dfm_tweets <- dfm_tweets0 %>%
dfm_trim(min_docfreq = 5) %>%
dfm_remove(c("@united", "@usairways", "@americanair", "@southwestair", "@jetblue", "@virginamerica", "flight", "amp", "aa", "get", "now", "united", "jetblue", "us"))
print("Document-Term Matrix after pre-processing")
print(paste0("Number of tweets:", dim(dfm_tweets)[1]))
print(paste0("Number of words:", dim(dfm_tweets)[2]))
#Text Preparation ----------
tweetCorpus <- corpus(tweets$text, docvars = tweets)
#turn corpus into document-term matrix; make everything lower-case, remove numbers, punctuation and stopwords; TF-IDF weighting; remove airline names
dfm_tweets0 <- tweetCorpus %>%
tokens(remove_numbers=T,
remove_punct=T,
include_docvars=T,
remove_symbols = T) %>%
tokens_remove(stopwords("en")) %>%
dfm(tolower=T)%>%
dfm_tfidf()
#eliminate rare words & other words
dfm_tweets <- dfm_tweets0 %>%
dfm_trim(min_docfreq = 5) %>%
dfm_remove(c("@united", "@usairways", "@americanair", "@southwestair", "@jetblue", "@virginamerica", "flight", "amp", "aa", "get", "now", "united", "jetblue", "us"))
print("Document-Term Matrix after pre-processing")
print(paste0("Number of tweets:", dim(dfm_tweets)[1]))
print(paste0("Number of words:", dim(dfm_tweets)[2]))
#overview
print(paste0("Most used words: ",textstat_frequency(dfm_tweets, force=T)[1:15])#most used words
#overview
print(paste0("Most used words: ",textstat_frequency(dfm_tweets, force=T)[1:15]))#most used words
#overview
print(paste0("Most used words: ",textstat_frequency(dfm_tweets, force=T)[1:15])) #most used words
print(textstat_frequency(dfm_tweets, force=T)[1:15])
textstat_frequency(dfm_tweets, force=T)[1:15]
View(dfm_tweets)
dfm_tweets
textstat_frequency(dfm_tweets, force=T)
frequency <- textstat_frequency(dfm_tweets, force=T)
frequency[1:15]
frequency[1:15,]
frequency[1:15,0]
frequency[1:15,1]
#overview
print(paste0("Most used words: ",textstat_frequency(dfm_tweets, force=T)[1:15,1])) #most used words
print(textstat_frequency(dfm_tweets, force=T)[1:15,1])
table(tweets$sentiment)#general proportion of each sentiment
tapply(tweets$sentiment, tweets$airline, table)#proportion of each sentiment per airline
knitr::opts_chunk$set(echo = TRUE)
#Text Preparation ----------
tweetCorpus <- corpus(tweets$text, docvars = tweets)
#turn corpus into document-term matrix; make everything lower-case, remove numbers, punctuation and stopwords; TF-IDF weighting; remove airline names
dfm_tweets0 <- tweetCorpus %>%
tokens(remove_numbers=T,
remove_punct=T,
include_docvars=T,
remove_symbols = T) %>%
tokens_remove(stopwords("en")) %>%
dfm(tolower=T)%>%
dfm_tfidf()
#eliminate rare words & other words
dfm_tweets <- dfm_tweets0 %>%
dfm_trim(min_docfreq = 5) %>%
dfm_remove(c("@united", "@usairways", "@americanair", "@southwestair", "@jetblue", "@virginamerica", "flight", "amp", "aa", "get", "now", "united", "jetblue", "us"))
print("Document-Term Matrix after pre-processing")
print(paste0("Number of tweets:", dim(dfm_tweets)[1]))
print(paste0("Number of words: ", dim(dfm_tweets)[2]))
print(paste0("Document-Term Matrix after pre-processing\n Number of tweets:", dim(dfm_tweets)[1]),"\n Number of words: ", dim(dfm_tweets)[2])))
print(paste0("Document-Term Matrix after pre-processing\n
Number of tweets:", dim(dfm_tweets)[1],
"\n Number of words: ", dim(dfm_tweets)[2])))
print(paste0("Document-Term Matrix after pre-processing\n
Number of tweets:", dim(dfm_tweets)[1],
"\n Number of words: ", dim(dfm_tweets)[2]))
cat("Document-Term Matrix after pre-processing\n
Number of tweets:", dim(dfm_tweets)[1],
"\n Number of words: ", dim(dfm_tweets)[2])
cat("Document-Term Matrix after pre-processing\n",
"Number of tweets:", dim(dfm_tweets)[1],
"\n Number of words: ", dim(dfm_tweets)[2])
# Get the frequency of terms in the document-term matrix
freq <- textstat_frequency(dfm_tweets, force = TRUE)
top_15_terms <- freq[1:15, 1]
cat("Top 15 most used words:\n", top_15_terms, sep = "\n")
# Compute the table of sentiment frequencies for each airline
sentiment_table <- tapply(tweets$sentiment, tweets$airline, table)
# Loop through each airline and print the proportion of each sentiment
for (airline in names(sentiment_table)) {
cat("Airline:", airline, "\n")
sentiment_count <- sentiment_table[[airline]]
sentiment_prop <- prop.table(sentiment_count)
print(sentiment_prop)
cat("\n")
}
View(tweets)
tweets <- tweets %>%
mutate(sentiment_long = ifelse(sentiment == 0, "Positive", "Negative"))
# Compute the table of sentiment frequencies for each airline
sentiment_table <- tapply(tweets$sentiment_long, tweets$airline, table)
# Loop through each airline and print the proportion of each sentiment
for (airline in names(sentiment_table)) {
cat("Airline:", airline, "\n")
sentiment_count <- sentiment_table[[airline]]
sentiment_prop <- prop.table(sentiment_count)
print(sentiment_prop)
cat("\n")
}
# Words most associated with negative & positive sentiment
bysentiment <- textstat_frequency(dfm_tweets,25,groups=sentiment_long, force=T)
# Words most associated with negative & positive sentiment
bysentiment <- textstat_frequency(dfm_tweets,25,groups=sentiment, force=T)
bysentiment$group[bysentiment$group=="0"] <- "Positive"
bysentiment$group[bysentiment$group=="1"] <- "Negative"
View(bysentiment)
# Words most associated with negative & positive sentiment
bysentiment <- textstat_frequency(dfm_tweets,25,groups=sentiment, force=T)
# Words most associated with negative & positive sentiment
bysentiment <- textstat_frequency(dfm_tweets,25,groups=sentiment, force=T) %>%
mutate(sentiment_long = ifelse(sentiment == 0, "Positive", "Negative"))
# Words most associated with negative & positive sentiment
bysentiment <- textstat_frequency(dfm_tweets,25,groups=sentiment, force=T) %>%
mutate(sentiment_long = ifelse(group == 0, "Positive", "Negative"))
positive_plot <- ggplot(bysentiment[bysentiment$group=="Positive"], aes(x=frequency,y=reorder(feature,frequency))) +
facet_wrap(~group, scales="free") +
geom_point() +
ylab("") +
xlab("Frequency")+
ggtitle("Words Most Associated with Positive Sentiment")
positive_plot
positive_plot <- ggplot(bysentiment[bysentiment$group=="Positive"], aes(x=frequency,y=reorder(feature,frequency)))
positive_plot <- ggplot(bysentiment[bysentiment$group=="Positive"], aes(x=frequency,y=reorder(feature,frequency))) +
facet_wrap(~group, scales="free")
positive_plot
ggplot(bysentiment[bysentiment$group=="Positive"],
aes(x=frequency,y=reorder(feature,frequency))) +
facet_wrap(~group, scales="free") +
geom_point() +
ylab("") +
xlab("Frequency")+
ggtitle("Words Most Associated with Positive Sentiment")
textplot_wordcloud(pos_tweets,
min_size = 0.5,
max_size=6,
color = "darkblue",
comparison=FALSE,
max_words=50)
#Wordclouds
pos_tweets <- dfm_tweets[dfm_tweets$sentiment == "0",]
textplot_wordcloud(pos_tweets,
min_size = 0.5,
max_size=6,
color = "darkblue",
comparison=FALSE,
max_words=50)
textplot_wordcloud(pos_tweets,
min_size = 0.5,
max_size=6,
color = "darkblue",
comparison=FALSE,
max_words=50)
pos_tweets
#Word Usage Analysis ----------
# Get the frequency of terms in the document-term matrix
freq <- textstat_frequency(dfm_tweets, force = TRUE)
top_15_terms <- freq[1:15, 1]
cat("Top 15 most used words:\n", top_15_terms, sep = "\n")
tweets <- tweets %>%
mutate(sentiment_long = ifelse(sentiment == 0, "Positive", "Negative"))
# Compute the table of sentiment frequencies for each airline
sentiment_table <- tapply(tweets$sentiment_long, tweets$airline, table)
# Loop through each airline and print the proportion of each sentiment
for (airline in names(sentiment_table)) {
cat("Airline:", airline, "\n")
sentiment_count <- sentiment_table[[airline]]
sentiment_prop <- prop.table(sentiment_count)
print(sentiment_prop)
cat("\n")
}
# Words most associated with negative & positive sentiment
bysentiment <- textstat_frequency(dfm_tweets,25,groups=sentiment, force=T) %>%
mutate(sentiment_long = ifelse(group == 0, "Positive", "Negative"))
#Wordclouds
pos_tweets <- dfm_tweets[dfm_tweets$sentiment == "0",]
textplot_wordcloud(pos_tweets,
min_size = 0.5,
max_size=6,
color = "darkblue",
comparison=FALSE,
max_words=50)
pos_tweets
neg_tweets <- dfm_tweets[dfm_tweets$sentiment == "1",]
textplot_wordcloud(neg_tweets,
min_size = 0.5,
max_size=6,
color = "red",
comparison=FALSE,
max_words=50)
neg_tweets
#most-used words for each airline (positive & negative)
byairline_pos <- textstat_frequency(pos_tweets, 5, groups=airline, force=TRUE)
ggplot(byairline_pos,
aes(x=frequency,y=reorder(feature,frequency))) +
facet_wrap(~group, scales="free") +
geom_point() +
ylab("") +
xlab("Frequency")
byairline_neg <- textstat_frequency(neg_tweets, 5, groups=airline, force=TRUE)
ggplot(byairline_neg,
aes(x=frequency,y=reorder(feature,frequency))) +
facet_wrap(~group, scales="free") +
geom_point() +
ylab("") +
xlab("Frequency")
#most-used words for each airline (positive & negative)
byairline_pos <- textstat_frequency(pos_tweets, 5, groups=airline, force=TRUE)
ggplot(byairline_pos,
aes(x=frequency,y=reorder(feature,frequency))) +
facet_wrap(~group, scales="free") +
geom_point() +
labs(title = "Positive Words for Each Airline",
y=" ",
x = "Frequency")
byairline_neg <- textstat_frequency(neg_tweets, 5, groups=airline, force=TRUE)
ggplot(byairline_neg,
aes(x=frequency,y=reorder(feature,frequency))) +
facet_wrap(~group, scales="free") +
geom_point() +
labs(title = "Negative Words for Each Airline",
y=" ",
x = "Frequency")
neg.words <- c("bad","worst", "terrible" , "cancelled","hold", "hours" ,"time", "delayed","gate", "phone")
pos.words <- c("thanks", "thank", "great", "love", "awesome","much", "good", "best", "appreciate", "amazing")
dico <- dictionary(list(negative = neg.words, positive = pos.words))
neg.words <- c("bad","worst", "terrible" , "cancelled","hold", "hours" ,"time", "delayed","gate", "phone")
pos.words <- c("thanks", "thank", "great", "love", "awesome","much", "good", "best", "appreciate", "amazing")
dico <- dictionary(list(negative = neg.words, positive = pos.words))
# Convert the dictionary to a data frame
dico_df <- data.frame(Sentiment = rep(names(dico), lengths(dico)),
Words = unlist(dico, use.names = FALSE))
# Print the table
print(dico_df)
neg.words <- c("bad","worst", "terrible" , "cancelled","hold", "hours" ,"time", "delayed","gate", "phone")
pos.words <- c("thanks", "thank", "great", "love", "awesome","much", "good", "best", "appreciate", "amazing")
dico <- dictionary(list(negative = neg.words, positive = pos.words))
# Create separate data frames for positive and negative words
pos_df <- data.frame(Sentiment = "Positive Words", Words = pos.words)
neg_df <- data.frame(Sentiment = "Negative Words", Words = neg.words)
# Combine the data frames
dico_df <- rbind(pos_df, neg_df)
# Add a title to the table
cat("Dictionary\n", dico_df)
neg.words <- c("bad","worst", "terrible" , "cancelled","hold", "hours" ,"time", "delayed","gate", "phone")
pos.words <- c("thanks", "thank", "great", "love", "awesome","much", "good", "best", "appreciate", "amazing")
dico <- dictionary(list(negative = neg.words, positive = pos.words))
# Create separate data frames for positive and negative words
pos_df <- data.frame(Sentiment = "Positive Words", Words = pos.words)
neg_df <- data.frame(Sentiment = "Negative Words", Words = neg.words)
# Combine the data frames
dico_df <- rbind(pos_df, neg_df)
# Add a title to the table
dico_df
neg.words <- c("bad","worst", "terrible" , "cancelled","hold", "hours" ,"time", "delayed","gate", "phone")
pos.words <- c("thanks", "thank", "great", "love", "awesome","much", "good", "best", "appreciate", "amazing")
dico <- dictionary(list(negative = neg.words, positive = pos.words))
# Create separate data frames for positive and negative words
pos_df <- data.frame(Sentiment = "Positive Words", Words = pos.words)
neg_df <- data.frame(Sentiment = "Negative Words", Words = neg.words)
# Combine the data frames
dico_df <- rbind(pos_df, neg_df)
# Create a gt table
gt_table <- dico_df %>%
gt() %>%
tab_header(title = "Dictionary") %>%
cols_label(Sentiment = "Sentiment Category", Words = "Words")
# Create a gt table
gt_table <- dico_df %>%
gt() %>%
tab_header(title = "Dictionary") %>%
set_header_labels(Sentiment = "Sentiment Category", Words = "Words")
library(gt)
# Create a gt table
gt_table <- dico_df %>%
gt() %>%
tab_header(title = "Dictionary") %>%
set_header_labels(Sentiment = "Sentiment Category", Words = "Words")
#create dictionary
neg.words <- c("bad","worst", "terrible" , "cancelled","hold", "hours" ,"time", "delayed","gate", "phone")
pos.words <- c("thanks", "thank", "great", "love", "awesome","much", "good", "best", "appreciate", "amazing")
dico <- dictionary(list(negative = neg.words, positive = pos.words))
#get sentiment score
sentiment_dico <- dfm_lookup(dfm_tweets,dictionary=dico)
sentiment_dico <- convert(sentiment_dico,to="data.frame")
View(sentiment_dico)
cat("Most negative Tweet: ", tweets$text[which.max(sentiment_dico$negative)])
cat("Most negative Tweet: ", tweets$text[which.max(sentiment_dico$negative)],
"\nMost positive Tweet: ", tweets$text[which.max(sentiment_dico$positive)])
cat("Most negative Tweet: ", tweets$text[which.max(sentiment_dico$negative)],
"\n\nMost positive Tweet: ", tweets$text[which.max(sentiment_dico$positive)])
#classify the tweets
sentiment_dico$score <- ifelse((sentiment_dico$positive - sentiment_dico$negative)>0,0,1)
#test dictionary based classifier
table(sentiment_dico$score,tweets$sentiment)
# Create the confusion matrix
confusion_matrix <- table(sentiment_dico$score, tweets$sentiment)
# Create the confusion matrix
confusion_matrix <- table(sentiment_dico$score, tweets$sentiment)
# Convert the confusion matrix to a data frame
confusion_df <- as.data.frame.matrix(confusion_matrix)
View(confusion_df)
# Create a heatmap using ggplot2
heatmap <- ggplot(data = confusion_df, aes(x = Var1, y = Var2, fill = Freq)) +
geom_tile() +
labs(title = "Confusion Matrix", x = "Predicted", y = "Actual", fill = "Frequency") +
theme_minimal() +
scale_fill_gradient(low = "lightblue", high = "darkblue")
heatmap
# Convert row names to a new column
confusion_df <- confusion_df %>%
rownames_to_column(var = "Predicted")
# Convert column names to a new row
confusion_df <- confusion_df %>%
pivot_longer(cols = -Predicted, names_to = "Actual", values_to = "Frequency")
# Convert the confusion matrix to a data frame
confusion_df <- as.data.frame.matrix(confusion_matrix)
confusion_df <- confusion_df %>%
#convert row names to a new column
rownames_to_column(var = "Predicted") %>%
#convert column names to a new row
pivot_longer(cols = -Predicted, names_to = "Actual", values_to = "Frequency")
confusion_df$Actual <- as.character(confusion_df$Actual)
# Convert factor levels to character
confusion_df$Predicted <- as.character(confusion_df$Predicted)
#create heatmap
heatmap <- ggplot(data = confusion_df, aes(x = Predicted, y = Actual, fill = Frequency)) +
geom_tile() +
labs(title = "Confusion Matrix", x = "Predicted", y = "Actual", fill = "Frequency") +
theme_minimal() +
scale_fill_gradient(low = "lightblue", high = "darkblue")
heatmap
#create heatmap
heatmap <- ggplot(data = confusion_df, aes(x = Predicted, y = Actual, fill = Frequency)) +
geom_tile() +
geom_text(size = 12, color = "white") +
labs(title = "Confusion Matrix", x = "Predicted", y = "Actual", fill = "Frequency") +
theme_minimal() +
scale_fill_gradient(low = "lightblue", high = "darkblue")
#create heatmap
heatmap <- ggplot(data = confusion_df, aes(x = Predicted, y = Actual, fill = Frequency)) +
geom_tile() +
geom_text(size = 12, color = "white") +
labs(title = "Confusion Matrix", x = "Predicted", y = "Actual", fill = "Frequency") +
theme_minimal() +
scale_fill_gradient(low = "lightblue", high = "darkblue")
heatmap
#create heatmap
heatmap <- ggplot(data = confusion_df, aes(x = Predicted, y = Actual, fill = Frequency, label = Frequency)) +
geom_tile() +
geom_text(size = 12, color = "white") +
labs(title = "Confusion Matrix", x = "Predicted", y = "Actual", fill = "Frequency") +
theme_minimal() +
scale_fill_gradient(low = "lightblue", high = "darkblue")
heatmap
#create heatmap
heatmap <- ggplot(data = confusion_df, aes(x = Predicted, y = Actual, fill = Frequency, label = Frequency)) +
geom_tile() +
geom_text(size = 12, color = "white") +
labs(title = "Confusion Matrix", x = "Predicted", y = "Actual", fill = "Frequency") +
theme_minimal() +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
guides(fill = FALSE)
# Create the confusion matrix
confusion_matrix <- table(sentiment_dico$score, tweets$sentiment)
# Convert the confusion matrix to a data frame
confusion_df <- as.data.frame.matrix(confusion_matrix)
confusion_df <- confusion_df %>%
#convert row names to a new column
rownames_to_column(var = "Predicted") %>%
#convert column names to a new row
pivot_longer(cols = -Predicted, names_to = "Actual", values_to = "Frequency")
# Convert factor levels to character
confusion_df$Predicted <- as.character(confusion_df$Predicted)
confusion_df$Actual <- as.character(confusion_df$Actual)
#create heatmap
heatmap <- ggplot(data = confusion_df, aes(x = Predicted, y = Actual, fill = Frequency, label = Frequency)) +
geom_tile() +
geom_text(size = 12, color = "white") +
labs(title = "Confusion Matrix", x = "Predicted", y = "Actual", fill = "Frequency") +
theme_minimal() +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
guides(fill = "none")
heatmap
setwd("~/Desktop/Blog/Blog-Articles/Wildfires-Canada/data/lpr_000b16a_e")
# Load data & package ----------
library(sf)
library(tidyverse)
# Visualization ---------------
shp = st_read("Canada-provinces.shp")
View(shp)
shp
